[[34m2023-04-30 20:49:48,223[0m] {[34mscheduler_job.py:[0m714} INFO[0m - Starting the scheduler[0m
[[34m2023-04-30 20:49:48,224[0m] {[34mscheduler_job.py:[0m719} INFO[0m - Processing each file at most -1 times[0m
[[34m2023-04-30 20:49:48,227[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2023-04-30 20:49:48,234[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 5997[0m
[[34m2023-04-30 20:49:48,237[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 20:49:48,254[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2023-04-30T20:49:48.291+0000] {manager.py:409} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2023-04-30 20:54:48,411[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 20:59:48,455[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:04:48,490[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:09:48,532[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:14:48,567[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:19:48,610[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:24:48,646[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:29:48,680[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:34:48,715[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:37:40,745[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:37:39.173828+00:00 [scheduled]>[0m
[[34m2023-04-30 21:37:40,745[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:37:40,746[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:37:39.173828+00:00 [scheduled]>[0m
[[34m2023-04-30 21:37:40,877[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:37:39.173828+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:37:40,877[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:37:39.173828+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:37:40,909[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:37:39.173828+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:37:42,230[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:37:43,154[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:37:39.173828+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:37:46,181[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:37:39.173828+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:37:46,188[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:37:39.173828+00:00, map_index=-1, run_start_date=2023-04-30 21:37:43.237971+00:00, run_end_date=2023-04-30 21:37:45.603897+00:00, run_duration=2.365926, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:37:40.746676+00:00, queued_by_job_id=1, pid=29540[0m
[[34m2023-04-30 21:37:46,373[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:37:39.173828+00:00 [scheduled]>[0m
[[34m2023-04-30 21:37:46,375[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:37:46,375[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:37:39.173828+00:00 [scheduled]>[0m
[[34m2023-04-30 21:37:46,378[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:37:39.173828+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:37:46,378[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:37:39.173828+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:37:46,407[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:37:39.173828+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:37:47,522[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:37:48,360[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:37:39.173828+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:37:49,407[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:37:39.173828+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:37:49,411[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:37:39.173828+00:00, map_index=-1, run_start_date=2023-04-30 21:37:48.429132+00:00, run_end_date=2023-04-30 21:37:48.813217+00:00, run_duration=0.384085, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:37:46.376322+00:00, queued_by_job_id=1, pid=29581[0m
[[34m2023-04-30 21:37:49,448[0m] {[34mdagrun.py:[0m586} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:37:39.173828+00:00: manual__2023-04-30T21:37:39.173828+00:00, state:running, queued_at: 2023-04-30 21:37:39.231772+00:00. externally triggered: True> failed[0m
[[34m2023-04-30 21:37:49,448[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:37:39.173828+00:00, run_id=manual__2023-04-30T21:37:39.173828+00:00, run_start_date=2023-04-30 21:37:40.527976+00:00, run_end_date=2023-04-30 21:37:49.448673+00:00, run_duration=8.920697, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:37:39.173828+00:00, data_interval_end=2023-04-30 21:37:39.173828+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:37:49,451[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:39:09,151[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:39:07.599803+00:00 [scheduled]>[0m
[[34m2023-04-30 21:39:09,152[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:39:09,152[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:39:07.599803+00:00 [scheduled]>[0m
[[34m2023-04-30 21:39:09,155[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:39:07.599803+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:39:09,156[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:39:07.599803+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:39:09,185[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:39:07.599803+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:39:10,445[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:39:11,345[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:39:07.599803+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:39:14,017[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:39:07.599803+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:39:14,021[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:39:07.599803+00:00, map_index=-1, run_start_date=2023-04-30 21:39:11.407445+00:00, run_end_date=2023-04-30 21:39:13.445298+00:00, run_duration=2.037853, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:39:09.153477+00:00, queued_by_job_id=1, pid=30083[0m
[[34m2023-04-30 21:39:14,162[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:39:07.599803+00:00 [scheduled]>[0m
[[34m2023-04-30 21:39:14,162[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:39:14,162[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:39:07.599803+00:00 [scheduled]>[0m
[[34m2023-04-30 21:39:14,164[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:39:07.599803+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:39:14,164[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:39:07.599803+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:39:14,193[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:39:07.599803+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:39:15,158[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:39:15,983[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:39:07.599803+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:39:17,038[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:39:07.599803+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:39:17,042[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:39:07.599803+00:00, map_index=-1, run_start_date=2023-04-30 21:39:16.046164+00:00, run_end_date=2023-04-30 21:39:16.466089+00:00, run_duration=0.419925, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:39:14.162968+00:00, queued_by_job_id=1, pid=30099[0m
[[34m2023-04-30 21:39:17,081[0m] {[34mdagrun.py:[0m586} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:39:07.599803+00:00: manual__2023-04-30T21:39:07.599803+00:00, state:running, queued_at: 2023-04-30 21:39:07.607830+00:00. externally triggered: True> failed[0m
[[34m2023-04-30 21:39:17,082[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:39:07.599803+00:00, run_id=manual__2023-04-30T21:39:07.599803+00:00, run_start_date=2023-04-30 21:39:08.998089+00:00, run_end_date=2023-04-30 21:39:17.082385+00:00, run_duration=8.084296, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:39:07.599803+00:00, data_interval_end=2023-04-30 21:39:07.599803+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:39:17,084[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:39:48,750[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:40:27,725[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:40:26.293455+00:00 [scheduled]>[0m
[[34m2023-04-30 21:40:27,725[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:40:27,726[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:40:26.293455+00:00 [scheduled]>[0m
[[34m2023-04-30 21:40:27,730[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:40:26.293455+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:40:27,730[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:40:26.293455+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:40:27,760[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:40:26.293455+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:40:29,089[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:40:30,191[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:40:26.293455+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:40:32,685[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:40:26.293455+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:40:32,689[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:40:26.293455+00:00, map_index=-1, run_start_date=2023-04-30 21:40:30.252776+00:00, run_end_date=2023-04-30 21:40:32.128479+00:00, run_duration=1.875703, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:40:27.727263+00:00, queued_by_job_id=1, pid=30584[0m
[[34m2023-04-30 21:40:33,213[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:40:26.293455+00:00 [scheduled]>[0m
[[34m2023-04-30 21:40:33,213[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:40:33,213[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:40:26.293455+00:00 [scheduled]>[0m
[[34m2023-04-30 21:40:33,215[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:40:26.293455+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:40:33,215[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:40:26.293455+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:40:33,243[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:40:26.293455+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:40:34,278[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:40:35,141[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:40:26.293455+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:40:36,139[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:40:26.293455+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:40:36,144[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:40:26.293455+00:00, map_index=-1, run_start_date=2023-04-30 21:40:35.205743+00:00, run_end_date=2023-04-30 21:40:35.559957+00:00, run_duration=0.354214, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:40:33.213985+00:00, queued_by_job_id=1, pid=30607[0m
[[34m2023-04-30 21:40:36,185[0m] {[34mdagrun.py:[0m586} ERROR[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:40:26.293455+00:00: manual__2023-04-30T21:40:26.293455+00:00, state:running, queued_at: 2023-04-30 21:40:26.299688+00:00. externally triggered: True> failed[0m
[[34m2023-04-30 21:40:36,185[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:40:26.293455+00:00, run_id=manual__2023-04-30T21:40:26.293455+00:00, run_start_date=2023-04-30 21:40:27.574623+00:00, run_end_date=2023-04-30 21:40:36.185700+00:00, run_duration=8.611077, state=failed, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:40:26.293455+00:00, data_interval_end=2023-04-30 21:40:26.293455+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:40:36,187[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:41:17,793[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:17,794[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:41:17,794[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:17,796[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:41:16.831491+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:41:17,796[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:17,823[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:19,142[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:41:20,075[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:41:16.831491+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:41:22,517[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:41:16.831491+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:41:22,521[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:41:16.831491+00:00, map_index=-1, run_start_date=2023-04-30 21:41:20.138011+00:00, run_end_date=2023-04-30 21:41:21.954724+00:00, run_duration=1.816713, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:41:17.794880+00:00, queued_by_job_id=1, pid=30871[0m
[[34m2023-04-30 21:41:22,650[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:22,650[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:41:22,651[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:22,652[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:41:16.831491+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:41:22,653[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:22,680[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:23,664[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:41:24,683[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:41:16.831491+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:41:25,687[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:41:16.831491+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:41:25,691[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:41:16.831491+00:00, map_index=-1, run_start_date=2023-04-30 21:41:24.749162+00:00, run_end_date=2023-04-30 21:41:25.118599+00:00, run_duration=0.369437, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:41:22.651567+00:00, queued_by_job_id=1, pid=30912[0m
[[34m2023-04-30 21:41:25,768[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:25,768[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:41:25,769[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:41:16.831491+00:00 [scheduled]>[0m
[[34m2023-04-30 21:41:25,770[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-04-30T21:41:16.831491+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-04-30 21:41:25,770[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:25,798[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:41:16.831491+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:41:26,790[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:41:27,679[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:41:16.831491+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:41:28,828[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-04-30T21:41:16.831491+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:41:28,832[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-04-30T21:41:16.831491+00:00, map_index=-1, run_start_date=2023-04-30 21:41:27.744578+00:00, run_end_date=2023-04-30 21:41:28.212265+00:00, run_duration=0.467687, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-04-30 21:41:25.769494+00:00, queued_by_job_id=1, pid=30927[0m
[[34m2023-04-30 21:41:28,906[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:41:16.831491+00:00: manual__2023-04-30T21:41:16.831491+00:00, state:running, queued_at: 2023-04-30 21:41:16.838606+00:00. externally triggered: True> successful[0m
[[34m2023-04-30 21:41:28,906[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:41:16.831491+00:00, run_id=manual__2023-04-30T21:41:16.831491+00:00, run_start_date=2023-04-30 21:41:17.713950+00:00, run_end_date=2023-04-30 21:41:28.906562+00:00, run_duration=11.192612, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:41:16.831491+00:00, data_interval_end=2023-04-30 21:41:16.831491+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:41:28,908[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:43:39,217[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:39,217[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:43:39,217[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:39,254[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:43:38.258236+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:43:39,254[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:39,287[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:40,578[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:43:41,424[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:43:38.258236+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:43:44,067[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:43:38.258236+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:43:44,073[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:43:38.258236+00:00, map_index=-1, run_start_date=2023-04-30 21:43:41.486167+00:00, run_end_date=2023-04-30 21:43:43.448872+00:00, run_duration=1.962705, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:43:39.218480+00:00, queued_by_job_id=1, pid=31815[0m
[[34m2023-04-30 21:43:44,216[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:44,217[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:43:44,217[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:44,218[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:43:38.258236+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:43:44,218[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:44,247[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:45,287[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:43:46,061[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:43:38.258236+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:43:47,043[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:43:38.258236+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:43:47,048[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:43:38.258236+00:00, map_index=-1, run_start_date=2023-04-30 21:43:46.126053+00:00, run_end_date=2023-04-30 21:43:46.486558+00:00, run_duration=0.360505, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:43:44.217655+00:00, queued_by_job_id=1, pid=31855[0m
[[34m2023-04-30 21:43:47,125[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:47,126[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:43:47,126[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:43:38.258236+00:00 [scheduled]>[0m
[[34m2023-04-30 21:43:47,128[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-04-30T21:43:38.258236+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-04-30 21:43:47,128[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:47,159[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:43:38.258236+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:43:48,089[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:43:49,155[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:43:38.258236+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:43:50,210[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-04-30T21:43:38.258236+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:43:50,214[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-04-30T21:43:38.258236+00:00, map_index=-1, run_start_date=2023-04-30 21:43:49.220393+00:00, run_end_date=2023-04-30 21:43:49.689483+00:00, run_duration=0.46909, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-04-30 21:43:47.126717+00:00, queued_by_job_id=1, pid=31876[0m
[[34m2023-04-30 21:43:50,703[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:43:38.258236+00:00: manual__2023-04-30T21:43:38.258236+00:00, state:running, queued_at: 2023-04-30 21:43:38.268075+00:00. externally triggered: True> successful[0m
[[34m2023-04-30 21:43:50,704[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:43:38.258236+00:00, run_id=manual__2023-04-30T21:43:38.258236+00:00, run_start_date=2023-04-30 21:43:39.109618+00:00, run_end_date=2023-04-30 21:43:50.704326+00:00, run_duration=11.594708, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:43:38.258236+00:00, data_interval_end=2023-04-30 21:43:38.258236+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:43:50,706[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:44:48,785[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:44:52,922[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:44:52,927[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:44:52,928[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:44:53,009[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:44:51.347568+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:44:53,009[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:44:53,038[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:44:54,048[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:44:54,960[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:44:51.347568+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:44:57,484[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:44:51.347568+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:44:57,488[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:44:51.347568+00:00, map_index=-1, run_start_date=2023-04-30 21:44:55.026152+00:00, run_end_date=2023-04-30 21:44:56.901334+00:00, run_duration=1.875182, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:44:52.929406+00:00, queued_by_job_id=1, pid=32318[0m
[[34m2023-04-30 21:44:57,600[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:44:57,601[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:44:57,601[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:44:57,603[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:44:51.347568+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:44:57,603[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:44:57,632[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:44:58,766[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:44:59,565[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:44:51.347568+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:45:00,655[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:44:51.347568+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:45:00,660[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:44:51.347568+00:00, map_index=-1, run_start_date=2023-04-30 21:44:59.628783+00:00, run_end_date=2023-04-30 21:45:00.078741+00:00, run_duration=0.449958, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:44:57.601941+00:00, queued_by_job_id=1, pid=32358[0m
[[34m2023-04-30 21:45:00,736[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:45:00,736[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:45:00,736[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:44:51.347568+00:00 [scheduled]>[0m
[[34m2023-04-30 21:45:00,738[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-04-30T21:44:51.347568+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-04-30 21:45:00,738[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:45:00,767[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:44:51.347568+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:45:01,726[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:45:02,613[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:44:51.347568+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:45:03,650[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-04-30T21:44:51.347568+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:45:03,654[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-04-30T21:44:51.347568+00:00, map_index=-1, run_start_date=2023-04-30 21:45:02.677476+00:00, run_end_date=2023-04-30 21:45:03.121915+00:00, run_duration=0.444439, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-04-30 21:45:00.736985+00:00, queued_by_job_id=1, pid=32379[0m
[[34m2023-04-30 21:45:03,724[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:44:51.347568+00:00: manual__2023-04-30T21:44:51.347568+00:00, state:running, queued_at: 2023-04-30 21:44:51.366793+00:00. externally triggered: True> successful[0m
[[34m2023-04-30 21:45:03,725[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:44:51.347568+00:00, run_id=manual__2023-04-30T21:44:51.347568+00:00, run_start_date=2023-04-30 21:44:52.695740+00:00, run_end_date=2023-04-30 21:45:03.725256+00:00, run_duration=11.029516, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:44:51.347568+00:00, data_interval_end=2023-04-30 21:44:51.347568+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:45:03,727[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:49:48,818[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:50:02,752[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:02,753[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:50:02,753[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:02,758[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2023-04-30T21:50:01.810582+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2023-04-30 21:50:02,758[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:02,788[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:04,150[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:50:04,999[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2023-04-30T21:50:01.810582+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:50:07,469[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.extract_task run_id=manual__2023-04-30T21:50:01.810582+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:50:07,473[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2023-04-30T21:50:01.810582+00:00, map_index=-1, run_start_date=2023-04-30 21:50:05.067875+00:00, run_end_date=2023-04-30 21:50:06.896281+00:00, run_duration=1.828406, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2023-04-30 21:50:02.754436+00:00, queued_by_job_id=1, pid=1951[0m
[[34m2023-04-30 21:50:07,583[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:07,583[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:50:07,583[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:07,585[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2023-04-30T21:50:01.810582+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2023-04-30 21:50:07,585[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:07,614[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:08,589[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:50:09,370[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2023-04-30T21:50:01.810582+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:50:10,390[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.transform_task run_id=manual__2023-04-30T21:50:01.810582+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:50:10,394[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2023-04-30T21:50:01.810582+00:00, map_index=-1, run_start_date=2023-04-30 21:50:09.437652+00:00, run_end_date=2023-04-30 21:50:09.795266+00:00, run_duration=0.357614, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2023-04-30 21:50:07.584075+00:00, queued_by_job_id=1, pid=1977[0m
[[34m2023-04-30 21:50:10,475[0m] {[34mscheduler_job.py:[0m360} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:10,475[0m] {[34mscheduler_job.py:[0m425} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2023-04-30 21:50:10,476[0m] {[34mscheduler_job.py:[0m511} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:50:01.810582+00:00 [scheduled]>[0m
[[34m2023-04-30 21:50:10,477[0m] {[34mscheduler_job.py:[0m550} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2023-04-30T21:50:01.810582+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2023-04-30 21:50:10,478[0m] {[34mbase_executor.py:[0m93} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:10,506[0m] {[34msequential_executor.py:[0m61} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2023-04-30T21:50:01.810582+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2023-04-30 21:50:11,613[0m] {[34mdagbag.py:[0m538} INFO[0m - Filling up the DagBag from /workspaces/airflow_etl/airflow/dags/etl_dag.py[0m
[[34m2023-04-30 21:50:12,389[0m] {[34mtask_command.py:[0m388} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2023-04-30T21:50:01.810582+00:00 [queued]> on host codespaces-f747d3[0m
[[34m2023-04-30 21:50:13,703[0m] {[34mscheduler_job.py:[0m602} INFO[0m - Executor reports execution of basic_etl_dag.load_task run_id=manual__2023-04-30T21:50:01.810582+00:00 exited with status success for try_number 1[0m
[[34m2023-04-30 21:50:13,707[0m] {[34mscheduler_job.py:[0m645} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2023-04-30T21:50:01.810582+00:00, map_index=-1, run_start_date=2023-04-30 21:50:12.461940+00:00, run_end_date=2023-04-30 21:50:13.047976+00:00, run_duration=0.586036, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2023-04-30 21:50:10.476599+00:00, queued_by_job_id=1, pid=1996[0m
[[34m2023-04-30 21:50:13,783[0m] {[34mdagrun.py:[0m607} INFO[0m - Marking run <DagRun basic_etl_dag @ 2023-04-30 21:50:01.810582+00:00: manual__2023-04-30T21:50:01.810582+00:00, state:running, queued_at: 2023-04-30 21:50:01.817461+00:00. externally triggered: True> successful[0m
[[34m2023-04-30 21:50:13,783[0m] {[34mdagrun.py:[0m658} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2023-04-30 21:50:01.810582+00:00, run_id=manual__2023-04-30T21:50:01.810582+00:00, run_start_date=2023-04-30 21:50:02.452910+00:00, run_end_date=2023-04-30 21:50:13.783865+00:00, run_duration=11.330955, state=success, external_trigger=True, run_type=manual, data_interval_start=2023-04-30 21:50:01.810582+00:00, data_interval_end=2023-04-30 21:50:01.810582+00:00, dag_hash=a98d5dbfb01936821518bb58bb3bd8e5[0m
[[34m2023-04-30 21:50:13,785[0m] {[34mdag.py:[0m3437} INFO[0m - Setting next_dagrun for basic_etl_dag to None, run_after=None[0m
[[34m2023-04-30 21:54:48,854[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 21:59:48,884[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2023-04-30 22:04:48,917[0m] {[34mscheduler_job.py:[0m1408} INFO[0m - Resetting orphaned tasks for active dag runs[0m
